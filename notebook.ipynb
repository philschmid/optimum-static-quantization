{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Quantization example with `optimum` for `distilbert`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install git+https://github.com/huggingface/optimum.git@main \"onnx\" \"onnxruntime>=1.9.0\" \"datasets>=1.2.1\" \"protobuf==3.20.1\"\n",
    "!pip install evaluate sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "lets define our `model_id` and `dataset`, will be used to statically quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id=\"mrm8488/distilroberta-finetuned-banking77\"\n",
    "dataset_id=\"banking77\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task=\"sequence-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 5.83k/5.83k [00:00<00:00, 2.86MB/s]\n",
      "Downloading: 100%|██████████| 319/319 [00:00<00:00, 312kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 878kB/s] \n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|██████████| 5.70k/5.70k [00:00<00:00, 5.98MB/s]\n",
      "Downloading: 100%|██████████| 256M/256M [00:05<00:00, 47.7MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=True, per_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Calibration of quantization ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-879d5f95a9fbe7e7.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 68.60ba/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "\n",
    "def preprocess_fn(ex, tokenizer):\n",
    "    return tokenizer(ex[\"text\"])\n",
    "\n",
    "# Create the calibration dataset\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    dataset_id,\n",
    "    preprocess_function=partial(preprocess_fn, tokenizer=quantizer.tokenizer),\n",
    "    num_samples=200,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the calibration configuration containing the parameters related to calibration.\n",
    "calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)\n",
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "ranges = quantizer.fit(\n",
    "    dataset=calibration_dataset,\n",
    "    calibration_config=calibration_config,\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    operators_to_quantize=qconfig.operators_to_quantize,\n",
    ")\n",
    "# remove temp augmented model again\n",
    "os.remove(\"augmented_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running quantization with calibrated ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-quantized.onnx')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantize the same way we did for dynamic quantization!\n",
    "quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 255.68 MB\n",
      "Quantized Model file size: 134.16 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,file_name=\"model-quantized.onnx\")\n",
    "\n",
    "clx = pipeline(\"text-classification\",model=model, tokenizer=quantizer.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'transfer_fee_charged', 'score': 0.3978312015533447}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clx(\"Please transfer 500€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Couldn't find a directory or a metric named 'accuracy' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "test_dataset= load_dataset(dataset_id,split=\"test\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def evaluate_model(pipe,dataset,metric,text_column=\"text\",label_column=\"label\"):\n",
    "    def eval(sample):\n",
    "        onnx = pipe(sample[text_column])\n",
    "        return {\"pred\": pipe.model.config.label2id[onnx[0][label_column]] ,\"ref\": sample[label_column]}\n",
    "\n",
    "    res = test_dataset.map(eval)\n",
    "    return metric.compute(references=res[\"ref\"], predictions=res[\"pred\"])\n",
    "\n",
    "pipe = pipeline(\"text-classification\",model=model_id)\n",
    "\n",
    "evaluate_model(pipe,test_dataset,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3080/3080 [00:59<00:00, 51.70ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3012987012987013}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\",model=model_id)\n",
    "\n",
    "evaluate_model(pipe,test_dataset,metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3080/3080 [00:17<00:00, 179.52ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2837662337662338}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(clx,test_dataset,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 13:11:28.726616837 [W:onnxruntime:, inference_session.cc:1546 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n",
      "Model producer not matched: Expect pytorch, Got onnx.quantize 0.1.0. Please specify correct --model_type parameter.\n",
      "Attention not fused\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-optimized.onnx')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model-quantized.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,file_name=\"model-optimized.onnx\")\n",
    "\n",
    "optimized_clx = pipeline(\"text-classification\",model=model, tokenizer=quantizer.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let’s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=\"What do I need to do to get my new card which I have requested 2 weeks ago?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "len(tokenizer(payload)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 46.81796969998686; Average latency (ms) - 45.68 +\\- 1.95;\n",
      "Quantized model: P95 latency (ms) - 38.94509134897817; Average latency (ms) - 26.37 +\\- 4.91;\n",
      "Optimized model: P95 latency (ms) - 24.78116525007863; Average latency (ms) - 24.40 +\\- 0.25;\n",
      "Improvement through quantization: 1.73x\n",
      "Improvement through optimization: 1.87x\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(1000):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_avg_ms\n",
    "\n",
    "\n",
    "vanilla_model=measure_latency(pipe)\n",
    "quantized_model=measure_latency(clx)\n",
    "optimized_model=measure_latency(optimized_clx)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Optimized model: {optimized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")\n",
    "print(f\"Improvement through optimization: {round(vanilla_model[1]/optimized_model[1],2)}x\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
