{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Quantization example with `optimum` for `distilbert`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install git+https://github.com/huggingface/optimum.git@main \"onnx\" \"onnxruntime>=1.9.0\" \"datasets>=1.2.1\" \"protobuf==3.20.1\"\n",
    "!pip install evaluate sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "lets define our `model_id` and `dataset`, will be used to statically quantize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id=\"mrm8488/distilroberta-finetuned-banking77\"\n",
    "dataset_id=\"banking77\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task=\"sequence-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 6.59k/6.59k [00:00<00:00, 5.72MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('onnx/tokenizer_config.json',\n",
       " 'onnx/special_tokens_map.json',\n",
       " 'onnx/vocab.json',\n",
       " 'onnx/merges.txt',\n",
       " 'onnx/added_tokens.json',\n",
       " 'onnx/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from onnxruntime.quantization import QuantFormat, QuantizationMode, QuantType\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=True,format=QuantFormat.QOperator,mode=QuantizationMode.QLinearOps, per_channel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Calibration of quantization ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "Loading cached shuffled indices for dataset at /home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b/cache-879d5f95a9fbe7e7.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Collecting tensor data and making histogram ...\n",
      "Finding optimal threshold for each tensor using percentile algorithm ...\n",
      "Number of tensors : 233\n",
      "Number of histogram bins : 2048\n",
      "Percentile : (0.006000000000000227,99.994)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from optimum.onnxruntime.configuration import AutoCalibrationConfig\n",
    "\n",
    "def preprocess_fn(ex, tokenizer):\n",
    "    return tokenizer(ex[\"text\"],padding=\"longest\")\n",
    "\n",
    "# Create the calibration dataset\n",
    "calibration_samples=256\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    dataset_id,\n",
    "    preprocess_function=partial(preprocess_fn, tokenizer=quantizer.tokenizer),\n",
    "    num_samples=calibration_samples,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Create the calibration configuration containing the parameters related to calibration.\n",
    "calibration_config = AutoCalibrationConfig.percentiles(calibration_dataset,percentile=99.994)\n",
    "# Perform the calibration step: computes the activations quantization ranges\n",
    "shards=16\n",
    "for i in range(shards):\n",
    "    shard = calibration_dataset.shard(shards, i)\n",
    "    quantizer.partial_fit(\n",
    "        dataset=shard,\n",
    "        calibration_config=calibration_config,\n",
    "        onnx_model_path=onnx_path / \"model.onnx\",\n",
    "        operators_to_quantize=qconfig.operators_to_quantize,\n",
    "        batch_size=calibration_samples//shards,\n",
    "        use_external_data_format=False,\n",
    "    )\n",
    "ranges = quantizer.compute_ranges()\n",
    "\n",
    "# remove temp augmented model again\n",
    "os.remove(\"augmented_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running quantization with calibrated ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime.preprocessors import QuantizationPreprocessor\n",
    "from optimum.onnxruntime.preprocessors.passes import (\n",
    "    ExcludeGeLUNodes,\n",
    "    ExcludeLayerNormNodes,\n",
    "    ExcludeNodeAfter,\n",
    "    ExcludeNodeFollowedBy,\n",
    ")\n",
    "\n",
    "# Create a quantization preprocessor to determine the nodes to exclude\n",
    "quantization_preprocessor = QuantizationPreprocessor()\n",
    "\n",
    "# Exclude the nodes constituting LayerNorm\n",
    "quantization_preprocessor.register_pass(ExcludeLayerNormNodes())\n",
    "# Exclude the nodes constituting GELU\n",
    "quantization_preprocessor.register_pass(ExcludeGeLUNodes())\n",
    "# Exclude the residual connection Add nodes\n",
    "quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Add\", \"Add\"))\n",
    "# Exclude the Add nodes following the Gather operator\n",
    "quantization_preprocessor.register_pass(ExcludeNodeAfter(\"Gather\", \"Add\"))\n",
    "# Exclude the Add nodes followed by the Softmax operator\n",
    "quantization_preprocessor.register_pass(ExcludeNodeFollowedBy(\"Add\", \"Softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-quantized.onnx')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantize the same way we did for dynamic quantization!\n",
    "quantizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_quantized_model_output_path=onnx_path / \"model-quantized.onnx\",\n",
    "    calibration_tensors_range=ranges,\n",
    "    quantization_config=qconfig,\n",
    "    # preprocessor=quantization_preprocessor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 313.54 MB\n",
      "Quantized Model file size: 192.03 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "quantized_model = os.path.getsize(onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "\n",
    "print(f\"Model file size: {size:.2f} MB\")\n",
    "print(f\"Quantized Model file size: {quantized_model:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,file_name=\"model-quantized.onnx\")\n",
    "\n",
    "clx = pipeline(\"text-classification\",model=model, tokenizer=quantizer.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'transfer_into_account', 'score': 0.12022580206394196}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clx(\"Please transfer 500€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset banking77 (/home/ubuntu/.cache/huggingface/datasets/banking77/default/1.1.0/aec0289529599d4572d76ab00c8944cb84f88410ad0c9e7da26189d31f62a55b)\n",
      "WARNING:evaluate.loading:Couldn't find a directory or a metric named 'accuracy' in this version. It was picked from the master branch on github instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "test_dataset= load_dataset(dataset_id,split=\"test\")\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def evaluate_model(pipe,dataset,metric,text_column=\"text\",label_column=\"label\"):\n",
    "    def eval(sample):\n",
    "        onnx = pipe(sample[text_column])\n",
    "        return {\"pred\": pipe.model.config.label2id[onnx[0][label_column]] ,\"ref\": sample[label_column]}\n",
    "\n",
    "    res = test_dataset.map(eval)\n",
    "    return metric.compute(references=res[\"ref\"], predictions=res[\"pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3080/3080 [01:05<00:00, 47.28ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8961038961038961}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-classification\",model=model_id)\n",
    "\n",
    "# evaluate_model(pipe,test_dataset,metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3080/3080 [00:19<00:00, 161.24ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.41883116883116883}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(clx,test_dataset,metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 15:37:08.900401438 [W:onnxruntime:, inference_session.cc:1546 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n",
      "WARNING:onnxruntime.transformers.optimizer:Model producer not matched: Expect pytorch, Got onnx.quantize 0.1.0. Please specify correct --model_type parameter.\n",
      "WARNING:onnx_model_bert:Attention not fused\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx/model-optimized.onnx')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model-quantized.onnx\",\n",
    "    onnx_optimized_model_output_path=onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=optimization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(onnx_path,file_name=\"model-optimized.onnx\")\n",
    "\n",
    "optimized_clx = pipeline(\"text-classification\",model=model, tokenizer=quantizer.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, let’s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload=\"What do I need to do to get my new card which I have requested 2 weeks ago?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length: 127\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np \n",
    "\n",
    "payload=\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend \"*2\n",
    "print(f'Payload sequence length: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(payload)\n",
    "    # Timed run\n",
    "    for _ in range(300):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(payload)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    time_p95_ms = 1000 * np.percentile(latencies,95)\n",
    "    return f\"P95 latency (ms) - {time_p95_ms}; Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f};\", time_avg_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model: P95 latency (ms) - 66.20852575069875; Average latency (ms) - 56.69 +\\- 9.12;\n",
      "Quantized model: P95 latency (ms) - 24.14308940024057; Average latency (ms) - 23.89 +\\- 0.34;\n",
      "Optimized model: P95 latency (ms) - 23.938274949250626; Average latency (ms) - 23.73 +\\- 0.65;\n",
      "Improvement through quantization: 2.37x\n",
      "Improvement through optimization: 2.39x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vanilla_model=measure_latency(pipe)\n",
    "quantized_model=measure_latency(clx)\n",
    "optimized_model=measure_latency(optimized_clx)\n",
    "\n",
    "print(f\"Vanilla model: {vanilla_model[0]}\")\n",
    "print(f\"Quantized model: {quantized_model[0]}\")\n",
    "print(f\"Optimized model: {optimized_model[0]}\")\n",
    "print(f\"Improvement through quantization: {round(vanilla_model[1]/quantized_model[1],2)}x\")\n",
    "print(f\"Improvement through optimization: {round(vanilla_model[1]/optimized_model[1],2)}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic counter part\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 6.59k/6.59k [00:00<00:00, 5.14MB/s]\n",
      "2022-06-02 13:46:50.475389962 [W:onnxruntime:, inference_session.cc:1546 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 313.54 MB\n",
      "Quantized Onnx Model file size: 222.52 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function evaluate_model.<locals>.eval at 0x7f51134b1160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 3080/3080 [00:19<00:00, 159.13ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8951298701298701}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "dynamic_onnx_path=Path(\"dynamic_onnx\")\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(dynamic_onnx_path)\n",
    "tokenizer.save_pretrained(dynamic_onnx_path)\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "dynamic_optimizer = ORTOptimizer.from_pretrained(model_id, feature=task)\n",
    "dynamic_optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "dynamic_optimizer.export(\n",
    "    onnx_model_path=onnx_path / \"model.onnx\",\n",
    "    onnx_optimized_model_output_path=dynamic_onnx_path / \"model-optimized.onnx\",\n",
    "    optimization_config=dynamic_optimization_config,\n",
    ")\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "dynamic_quantizer = ORTQuantizer.from_pretrained(model_id, feature=task)\n",
    "dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "dynamic_quantizer.export(\n",
    "    onnx_model_path=dynamic_onnx_path / \"model-optimized.onnx\",\n",
    "    onnx_quantized_model_output_path=dynamic_onnx_path / \"model-quantized.onnx\",\n",
    "    quantization_config=dqconfig,\n",
    ")\n",
    "\n",
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(dynamic_onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(dynamic_onnx_path / \"model-quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model = ORTModelForSequenceClassification.from_pretrained(dynamic_onnx_path,file_name=\"model-quantized.onnx\")\n",
    "\n",
    "dynamic_clx = pipeline(\"text-classification\",model=model, tokenizer=dynamic_quantizer.tokenizer)\n",
    "\n",
    "evaluate_model(dynamic_clx,test_dataset,metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P95 latency (ms) - 39.63735170000291; Average latency (ms) - 39.48 +\\\\- 0.45;'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_latency(dynamic_clx)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a2c4b191d1ae843dde5cb5f4d1f62fa892f6b79b0f9392a84691e890e33c5a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
